{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+yDfTsTeYnoGm+JEM3QDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanislavlia/DeepLearning.AI-specialization/blob/main/DeepNN_regression_california_house_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RclIfBFaVTAg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"sample_data/california_housing_train.csv\")"
      ],
      "metadata": {
        "id": "zUTOVxYPV29b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_vIq3dsWBO-",
        "outputId": "8558c49c-d49b-4274-cfb6-4ed6a713e7a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling values to improve convergence\n",
        "stdscaler = StandardScaler()\n",
        "\n",
        "train_df_scaled = pd.DataFrame(stdscaler.fit_transform(train_df), columns = train_df.columns)"
      ],
      "metadata": {
        "id": "T0-Q8t89WO4B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df_scaled.drop(\"median_house_value\", axis=1).values.T\n",
        "y_train = train_df_scaled[\"median_house_value\"].values.reshape(1, -1)\n"
      ],
      "metadata": {
        "id": "WfLlZdm2W3p5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#My class of DeepNN\n",
        "\n",
        "class DeepNN():\n",
        "    def __init__(self, X, Y, dims_of_layers, activations, alpha = 0.01, l2_reg = 0):\n",
        "        #dims_of_layers - list of number of units in each layer (first element - num of features in input)\n",
        "        #activations - activation function applied to each layer\n",
        "\n",
        "        #dims_of_layers[0] - n of features in input\n",
        "        #activations[0] - activation for first hidden layer\n",
        "        #we support only 3 activation funcs: linear, sigmoid, relu\n",
        "\n",
        "        #X.shape should be (n_features, m_examples)\n",
        "        #Y.shape should be (1, m_examples)\n",
        "\n",
        "        #regularization - regularization rate\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        self.m_examples = X.shape[1]\n",
        "\n",
        "        self.dims_of_layers = dims_of_layers\n",
        "        self.n_layers = len(activations)\n",
        "\n",
        "        self.activations = activations\n",
        "        self.params = dict()\n",
        "\n",
        "        self.learning_history = []\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        #setting cache dicts for backpropogation\n",
        "\n",
        "        self.cache = dict()\n",
        "\n",
        "    def initialize_params(self):\n",
        "      #Now, we are going to use He-initialization of weights\n",
        "\n",
        "        for i in range(1, len(self.activations) + 1):\n",
        "\n",
        "            #setting parameters layer by layer\n",
        "            self.params[\"W\" + str(i)] = np.random.randn(self.dims_of_layers[i], self.dims_of_layers[i-1])\n",
        "            #multiplying by constant according to He init\n",
        "            self.params[\"W\" + str(i)] *=  np.sqrt(2 / (self.dims_of_layers[i-1]))\n",
        "\n",
        "\n",
        "            self.params[\"b\" + str(i)] = np.zeros((self.dims_of_layers[i], 1))\n",
        "\n",
        "\n",
        "\n",
        "    def activation(self, Z, function=\"linear\"):\n",
        "        if function == \"linear\":\n",
        "            return Z\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return Z * (Z > 0)\n",
        "\n",
        "\n",
        "    def deriv_activation(self, Z, function):\n",
        "        if function == \"linear\":\n",
        "            return 1.\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            sigm_z = self.activation(Z, \"sigmoid\")\n",
        "\n",
        "            return sigm_z *(1 - sigm_z)\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return 1. * (Z > 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propogation(self, X):\n",
        "\n",
        "        #X.shape = (n_features, m_examples)\n",
        "\n",
        "        A_prev = X\n",
        "        self.cache[\"A0\"]  = A_prev\n",
        "\n",
        "        Z_current = np.dot(self.params[\"W1\"], A_prev) + self.params[\"b1\"]\n",
        "        A_current = self.activation(Z_current, function=self.activations[0])\n",
        "\n",
        "        self.cache[\"Z1\"] = Z_current\n",
        "        self.cache[\"A1\"] = A_current\n",
        "\n",
        "        for i in range(1, len(self.activations)):\n",
        "            A_prev = A_current\n",
        "\n",
        "            #A_prev - cache[\"A\" + str(i)]\n",
        "            Z_current = np.dot(self.params[\"W\" + str(i+1)], A_prev) + self.params[\"b\" + str(i+1)]\n",
        "            A_current = self.activation(Z_current, function=self.activations[i])\n",
        "\n",
        "            #keeping values in cache for backprop\n",
        "            self.cache[\"Z\" + str(i+1)] = Z_current\n",
        "            self.cache[\"A\" + str(i+1)] = A_current\n",
        "\n",
        "\n",
        "        predictions = A_current\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def compute_reg_penalty(self):\n",
        "\n",
        "      penalty = 0\n",
        "      for l in range(1, self.n_layers + 1):\n",
        "        W_l = self.params[\"W\" + str(l)]\n",
        "        penalty += np.sum(np.square(W_l))\n",
        "\n",
        "      penalty = penalty * (self.l2_reg / (2 * self.m_examples))\n",
        "\n",
        "      return penalty\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_cost(self, predictions, cost_function):\n",
        "        #leave cost func as a parameter\n",
        "        #so that we can use it futher not only for classification\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #lets cut off a  tiny constant to avoid log0 problem\n",
        "            epsilon = 10 ** -15\n",
        "\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            #BinaryCrossEntropy\n",
        "\n",
        "            cost = (self.Y * np.log(predictions) +\n",
        "                    (1 - self.Y) * np.log(1 - (predictions)) ) * (-1 / self.m_examples)\n",
        "\n",
        "            cost = np.sum(cost, axis=1, keepdims=True) #sum up the columns\n",
        "\n",
        "        if cost_function == \"mse\":\n",
        "\n",
        "            cost = np.sum(((predictions - self.Y) ** 2) * (2 / self.m_examples), axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        #computing regularization penalty\n",
        "        if self.l2_reg> 0:\n",
        "\n",
        "          reg_penalty = self.compute_reg_penalty()\n",
        "          cost += reg_penalty\n",
        "\n",
        "\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def deriv_of_cost(self, predictions, cost_function):\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #avoiding division by zero\n",
        "            epsilon = 10 ** -15\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            dAL = (predictions - self.Y) / (predictions * (1 - predictions))\n",
        "\n",
        "        if cost_function == \"mse\":\n",
        "            dAL = (predictions - self.Y)\n",
        "\n",
        "        return dAL\n",
        "\n",
        "\n",
        "    def back_propogation(self, predictions, cost_func):\n",
        "\n",
        "        L = self.n_layers\n",
        "\n",
        "        grads_cache = dict()\n",
        "\n",
        "        for i in range(L, 0, -1):\n",
        "            if i == L:\n",
        "                dA_i = self.deriv_of_cost(predictions, cost_function=cost_func)\n",
        "            else:\n",
        "\n",
        "                dA_i = np.dot(self.params[\"W\" + str(i+1)].T, grads_cache[\"dZ\" + str(i+1)]) #ERROR MuSt be here\n",
        "                #print(\"i =\", i)\n",
        "                # print(\"W=\", self.params[\"W\" + str(i+1)] )\n",
        "                # print(\"dZ_next =\", grads_cache[\"dZ\" + str(i+1)])\n",
        "\n",
        "                # print(\"W = \", self.params[\"W\" + str(i+1)].T.shape)\n",
        "                # print(\"W shape \", )\n",
        "                # print(\"dZ shape\" + str(i),  grads_cache[\"dZ\" + str(i+1)].shape)\n",
        "                #print(\"dA\", dA_i)\n",
        "\n",
        "\n",
        "            grads_cache[\"dA\" + str(i)] = dA_i\n",
        "            activation_i = self.activations[i-1]\n",
        "\n",
        "            Z_i = self.cache[\"Z\" + str(i)]\n",
        "            A_prev = self.cache[\"A\" + str(i-1)]\n",
        "            W_i = self.params[\"W\" + str(i)]\n",
        "\n",
        "            #print(\"Activation = \", activation_i)\n",
        "            dZ_i = dA_i * self.deriv_activation(Z_i, activation_i)\n",
        "\n",
        "            #print(\"Sum of dZ_i\", np.sum(dZ_i))\n",
        "\n",
        "            #computing derivs for W, b\n",
        "\n",
        "            #L2 regularization term\n",
        "            l2_term = (self.l2_reg / self.m_examples) * W_i\n",
        "\n",
        "            dW_i = (1 / self.m_examples) * np.dot(dZ_i, A_prev.T) + l2_term\n",
        "            db_i = (1/ self.m_examples) * np.sum(dZ_i, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "            #storing gradients\n",
        "            grads_cache[\"dZ\" + str(i)] = dZ_i\n",
        "            grads_cache[\"dW\" + str(i)] = dW_i\n",
        "            grads_cache[\"db\" + str(i)] = db_i\n",
        "\n",
        "        return grads_cache\n",
        "\n",
        "    def update_params(self, grads):\n",
        "\n",
        "        for i in range(1, self.n_layers + 1):\n",
        "\n",
        "            #updating by Gradient Descent\n",
        "            self.params[\"W\" + str(i)] -=  self.alpha * grads[\"dW\" + str(i)]\n",
        "            self.params[\"b\" + str(i)] -= self.alpha * grads[\"db\" + str(i)]\n",
        "\n",
        "\n",
        "    def fit(self, epochs=100, cost_func=\"mse\", debug=False):\n",
        "\n",
        "        #fitting process\n",
        "\n",
        "        #initialize random params\n",
        "\n",
        "        history = []\n",
        "        self.initialize_params()\n",
        "\n",
        "\n",
        "        for epoch in range(0, epochs + 1):\n",
        "\n",
        "            predictions = self.forward_propogation(self.X)\n",
        "\n",
        "            #computing cost function\n",
        "            cost = np.round(self.compute_cost(predictions, cost_func), 6)\n",
        "            history.append(cost)\n",
        "\n",
        "\n",
        "            if (epoch % max(1 , (epochs // 20)) == 0):\n",
        "              print(\"Epoch #{},  {} == {}\".format(epoch, cost_func, cost))\n",
        "\n",
        "            #computing gradients\n",
        "            grads = self.back_propogation(predictions, cost_func=cost_func)\n",
        "            if debug == True:\n",
        "              print(grads)\n",
        "\n",
        "            #update params using Gradient Descent\n",
        "            self.update_params(grads)\n",
        "\n",
        "        self.history = history"
      ],
      "metadata": {
        "id": "u47L-cUaW4-Q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the network\n",
        "\n",
        "layers = [X_train.shape[0], 10, 5, 1]\n",
        "activations = [\"relu\", \"relu\", \"linear\"]\n",
        "\n",
        "model = DeepNN(X_train, y_train, layers, activations, alpha = 0.1, l2_reg = 0.1)\n",
        "model.fit(epochs=20000, cost_func=\"mse\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjFWCTQgXvlm",
        "outputId": "0cf281fa-c0ff-40a6-c2cb-03fb5b5dddc1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0,  mse == [[7.550531]]\n",
            "Epoch #1000,  mse == [[0.545248]]\n",
            "Epoch #2000,  mse == [[0.520322]]\n",
            "Epoch #3000,  mse == [[0.506265]]\n",
            "Epoch #4000,  mse == [[0.499331]]\n",
            "Epoch #5000,  mse == [[0.499682]]\n",
            "Epoch #6000,  mse == [[0.492485]]\n",
            "Epoch #7000,  mse == [[0.487453]]\n",
            "Epoch #8000,  mse == [[0.485079]]\n",
            "Epoch #9000,  mse == [[0.481657]]\n",
            "Epoch #10000,  mse == [[0.479333]]\n",
            "Epoch #11000,  mse == [[0.471976]]\n",
            "Epoch #12000,  mse == [[0.468106]]\n",
            "Epoch #13000,  mse == [[0.459757]]\n",
            "Epoch #14000,  mse == [[0.453406]]\n",
            "Epoch #15000,  mse == [[0.448572]]\n",
            "Epoch #16000,  mse == [[0.446813]]\n",
            "Epoch #17000,  mse == [[0.441419]]\n",
            "Epoch #18000,  mse == [[0.441386]]\n",
            "Epoch #19000,  mse == [[0.440426]]\n",
            "Epoch #20000,  mse == [[0.439794]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "train_predictions = model.forward_propogation(X_train)\n",
        "print(\"R^2 score on train set = \", r2_score(y_train.T, train_predictions.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUqxvGN-Ygfr",
        "outputId": "29b82774-c4a3-4583-8b9e-b50738876db9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score on train set =  0.7801732724694097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Uploading test data on housing in California**\n",
        "\n",
        "(note) These datasets are available by default in Google Colab"
      ],
      "metadata": {
        "id": "hrNUOQhZeRNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
        "\n",
        "test_df_scaled = pd.DataFrame(stdscaler.transform(test_df), columns = test_df.columns)\n",
        "\n",
        "X_test = test_df_scaled.drop(\"median_house_value\", axis=1).values.T\n",
        "y_test = test_df_scaled[\"median_house_value\"].values.reshape(1, -1)"
      ],
      "metadata": {
        "id": "X0aqZ_Oxed3t"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.forward_propogation(X_test)"
      ],
      "metadata": {
        "id": "v7deh5vnfLsH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R^2 score on test set = \", r2_score(y_test.T, predictions.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQOadfqWfmxn",
        "outputId": "b19c2653-71e4-4e0d-fcf7-4c6224cea0d8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score on test set =  0.7455538017150225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DcNx_bx3gCzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}