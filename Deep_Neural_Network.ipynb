{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0c11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d30864fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN():\n",
    "    def __init__(self, X, Y, dims_of_layers, activations, alpha = 0.01):\n",
    "        #dims_of_layers - list of number of units in each layer (first element - num of features in input)\n",
    "        #activations - activation function applied to each layer\n",
    "        \n",
    "        #dims_of_layers[0] - n of features in input\n",
    "        #activations[0] - activation for first hidden layer\n",
    "        #we support only 3 activation funcs: linear, sigmoid, relu\n",
    "        \n",
    "        #X.shape should be (n_features, m_examples)\n",
    "        #Y.shape should be (1, m_examples)\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.m_examples = X.shape[1]\n",
    "        \n",
    "        self.dims_of_layers = dims_of_layers\n",
    "        self.n_layers = len(activations)\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.params = dict()\n",
    "        \n",
    "        self.learning_history = []\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        #setting cache dicts for backpropogation\n",
    "        \n",
    "        self.cache = dict()\n",
    "            \n",
    "    def initialize_params(self):\n",
    "        \n",
    "        for i in range(1, len(self.activations) + 1):\n",
    "            \n",
    "            #setting parameters layer by layer\n",
    "            self.params[\"W\" + str(i)] = np.random.randn(self.dims_of_layers[i], self.dims_of_layers[i-1])\n",
    "            self.params[\"b\" + str(i)] = np.zeros((self.dims_of_layers[i], 1))\n",
    "            \n",
    "     \n",
    "    \n",
    "    def activation(self, Z, function=\"linear\"):\n",
    "        if function == \"linear\":\n",
    "            return Z\n",
    "        \n",
    "        if function == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "        \n",
    "        if function == \"relu\":\n",
    "            return Z * (Z > 0)\n",
    "        \n",
    "        \n",
    "    def deriv_activation(self, Z, function):\n",
    "        if function == \"linear\":\n",
    "            return 1.\n",
    "    \n",
    "        if function == \"sigmoid\":\n",
    "            sigm_z = self.activation(Z, \"sigmoid\")\n",
    "            \n",
    "            return sigm_z *(1 - sigm_z)\n",
    "        \n",
    "        if function == \"relu\":\n",
    "            return 1. * (Z > 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward_propogation(self, X):\n",
    "        \n",
    "        #X.shape = (n_features, m_examples)\n",
    "        \n",
    "        A_prev = X\n",
    "        self.cache[\"A0\"]  = A_prev\n",
    "        \n",
    "        Z_current = np.dot(self.params[\"W1\"], A_prev) + self.params[\"b1\"]\n",
    "        A_current = self.activation(Z_current, function=self.activations[0])\n",
    "        \n",
    "        self.cache[\"Z1\"] = Z_current\n",
    "        self.cache[\"A1\"] = A_current\n",
    "        \n",
    "        for i in range(1, len(self.activations)):\n",
    "            A_prev = A_current\n",
    "            \n",
    "            #A_prev - cache[\"A\" + str(i)]\n",
    "            Z_current = np.dot(self.params[\"W\" + str(i+1)], A_prev)\n",
    "            A_current = self.activation(Z_current, function=self.activations[i])\n",
    "            \n",
    "            #keeping values in cache for backprop\n",
    "            self.cache[\"Z\" + str(i+1)] = Z_current\n",
    "            self.cache[\"A\" + str(i+1)] = A_current            \n",
    "\n",
    "            \n",
    "        predictions = A_current\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def compute_cost(self, predictions, cost_function=\"cross_entropy\"):\n",
    "        #leave cost func as a parameter \n",
    "        #so that we can use it futher not only for classification\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if cost_function == \"cross_entropy\":\n",
    "            \n",
    "            #lets cut off a  tiny constant to avoid log0 problem\n",
    "            epsilon = 10 ** -15\n",
    "            \n",
    "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
    "            \n",
    "            #BinaryCrossEntropy\n",
    "        \n",
    "            cost = (self.Y * np.log(predictions) + \n",
    "                    (1 - self.Y) * np.log(1 - (predictions)) * (-1 / self.m_examples))\n",
    "            \n",
    "            cost = np.sum(cost, axis=1, keepdims=True) #sum up the columns\n",
    "            \n",
    "        #we will have more cost functions here later...\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def deriv_of_cost(self, predictions, cost_function=\"cross_entropy\"):\n",
    "        \n",
    "        if cost_function == \"cross_entropy\":\n",
    "            \n",
    "            #avoiding division by zero\n",
    "            epsilon = 10 ** -15\n",
    "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
    "            \n",
    "            dAL = (predictions - self.Y) / (predictions * (1 - predictions))\n",
    "            \n",
    "        #we will have more cost functions here later...\n",
    "        \n",
    "        return dAL\n",
    "            \n",
    "    \n",
    "    def back_propogation(self, predictions):\n",
    "        \n",
    "        L = self.n_layers\n",
    "        \n",
    "        grads_cache = dict()\n",
    "\n",
    "        for i in range(L, 0, -1):\n",
    "            if i == L:\n",
    "                dA_i = self.deriv_of_cost(predictions)\n",
    "            else:\n",
    "                \n",
    "                dA_i = np.dot(self.params[\"W\" + str(i+1)].T, grads_cache[\"dZ\" + str(i+1)])\n",
    "    \n",
    "    \n",
    "            grads_cache[\"dA\" + str(i)] = dA_i\n",
    "            activation_i = self.activations[-i]\n",
    "            \n",
    "            Z_i = self.cache[\"Z\" + str(i)]\n",
    "            A_prev = self.cache[\"A\" + str(i-1)]\n",
    "            W_i = self.params[\"W\" + str(i)]\n",
    "            \n",
    "            dZ_i = dA_i * self.deriv_activation(Z_i, activation_i)\n",
    "            \n",
    "            #computing derivs for W, b\n",
    "            dW_i = (1 / self.m_examples) * np.dot(dZ_i, A_prev.T)\n",
    "            db_i = (1/ self.m_examples) * np.sum(dZ_i, axis=1, keepdims=True)\n",
    "            \n",
    "            \n",
    "            #storing gradients\n",
    "            grads_cache[\"dZ\" + str(i)] = dZ_i\n",
    "            grads_cache[\"dW\" + str(i)] = dW_i\n",
    "            grads_cache[\"db\" + str(i)] = db_i\n",
    "            \n",
    "        return grads_cache\n",
    "             \n",
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "291f4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [3, 4, 2, 1]\n",
    "activation = [\"relu\", \"relu\", \"sigmoid\"]\n",
    "\n",
    "X = np.random.randn(3, 6)\n",
    "y = np.array([[1,1,1,0,0,0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "181f2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepnn = DeepNN(X, y, dims, activation)\n",
    "deepnn.initialize_params()\n",
    "predictions = deepnn.forward_propogation(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f804f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A0': array([[-1.0039288 , -2.61528105,  0.99282464, -1.21592424,  0.74263428,\n",
       "         -0.76234171],\n",
       "        [-0.89761618, -0.94736143,  0.463101  ,  0.02540576,  0.29786585,\n",
       "          0.5660561 ],\n",
       "        [ 1.74704228, -0.31632605, -0.24545828,  0.57849373, -1.00241947,\n",
       "          0.99948863]]),\n",
       " 'Z1': array([[-0.28136488,  0.84093675, -0.20114913,  0.22929381,  0.08370022,\n",
       "          0.01716013],\n",
       "        [ 0.18683092,  6.66498489, -2.06516028,  1.47561334, -0.2107284 ,\n",
       "         -0.64673994],\n",
       "        [-1.72353776, -2.5904964 ,  1.12433884, -0.33104613,  0.67518199,\n",
       "          0.80161828],\n",
       "        [-1.66975996, -0.39556833,  0.44512845,  0.57071072,  0.43408338,\n",
       "          1.30214053]]),\n",
       " 'A1': array([[-0.        ,  0.84093675, -0.        ,  0.22929381,  0.08370022,\n",
       "          0.01716013],\n",
       "        [ 0.18683092,  6.66498489, -0.        ,  1.47561334, -0.        ,\n",
       "         -0.        ],\n",
       "        [-0.        , -0.        ,  1.12433884, -0.        ,  0.67518199,\n",
       "          0.80161828],\n",
       "        [-0.        , -0.        ,  0.44512845,  0.57071072,  0.43408338,\n",
       "          1.30214053]]),\n",
       " 'Z2': array([[ 0.20879256,  6.7676691 ,  3.3747823 ,  1.18440619,  1.87730449,\n",
       "          1.91073014],\n",
       "        [ 0.1521427 ,  6.21821327, -1.02063834,  2.15101936, -0.31977742,\n",
       "          0.55461733]]),\n",
       " 'A2': array([[ 0.20879256,  6.7676691 ,  3.3747823 ,  1.18440619,  1.87730449,\n",
       "          1.91073014],\n",
       "        [ 0.1521427 ,  6.21821327, -0.        ,  2.15101936, -0.        ,\n",
       "          0.55461733]]),\n",
       " 'Z3': array([[ 0.21154347,  8.88904453, -0.46452736,  3.23413317, -0.25840461,\n",
       "          0.61291651]]),\n",
       " 'A3': array([[0.55268952, 0.99986213, 0.38591236, 0.96209876, 0.43575593,\n",
       "         0.64860581]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepnn.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b83aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA3': array([[-1.8093341 , -1.00013789, -2.59126192, 26.3843582 ,  1.7722827 ,\n",
       "          2.84580687]]),\n",
       " 'dZ3': array([[-1.8093341 , -1.00013789, -0.        , 26.3843582 ,  0.        ,\n",
       "          2.84580687]]),\n",
       " 'dW3': array([[4.92349804, 8.63954191]]),\n",
       " 'db3': array([[4.40344885]]),\n",
       " 'dA2': array([[ 0.24904871,  0.1376656 ,  0.        , -3.63171761,  0.        ,\n",
       "         -0.39171568],\n",
       "        [-2.85752995, -1.57954464,  0.        , 41.66952577,  0.        ,\n",
       "          4.49445925]]),\n",
       " 'dZ2': array([[ 0.24904871,  0.1376656 ,  0.        , -3.63171761,  0.        ,\n",
       "         -0.39171568],\n",
       "        [-2.85752995, -1.57954464,  0.        , 41.66952577,  0.        ,\n",
       "          4.49445925]]),\n",
       " 'dW2': array([[-0.12061403, -0.73249031, -0.05233441, -0.43045484],\n",
       "        [ 1.38389877,  8.40443199,  0.60047345,  4.93894374]]),\n",
       " 'db2': array([[-0.60611983],\n",
       "        [ 6.95448507]]),\n",
       " 'dA1': array([[ -2.8884116 ,  -1.59661496,   0.        ,  42.11985305,\n",
       "           0.        ,   4.54303138],\n",
       "        [ -2.04865879,  -1.13242838,   0.        ,  29.87427666,\n",
       "           0.        ,   3.22222815],\n",
       "        [  4.84427705,   2.67775036,   0.        , -70.64098395,\n",
       "           0.        ,  -7.61930974],\n",
       "        [ -3.79580095,  -2.0981887 ,   0.        ,  55.35172971,\n",
       "           0.        ,   5.97021658]]),\n",
       " 'dZ1': array([[-7.07997839e-01, -3.36140493e-01,  0.00000000e+00,\n",
       "          1.03927622e+01,  0.00000000e+00,  1.13567424e+00],\n",
       "        [-5.07721192e-01, -1.43991891e-03,  0.00000000e+00,\n",
       "          4.52479036e+00,  0.00000000e+00,  7.26863739e-01],\n",
       "        [ 6.22437388e-01,  1.73751578e-01,  0.00000000e+00,\n",
       "         -1.71850950e+01,  0.00000000e+00, -1.62884150e+00],\n",
       "        [-5.06161790e-01, -5.04551142e-01,  0.00000000e+00,\n",
       "          1.27696023e+01,  0.00000000e+00,  1.00354813e+00]]),\n",
       " 'dW1': array([[-1.98545034,  0.31014136,  1.00277818],\n",
       "        [-0.92373985,  0.16391738,  0.40958333],\n",
       "        [ 3.50970257, -0.34698857, -1.75616934],\n",
       "        [-2.4107037 ,  0.30413621,  1.27758108]]),\n",
       " 'db1': array([[ 1.74738301],\n",
       "        [ 0.7904155 ],\n",
       "        [-3.00295793],\n",
       "        [ 2.12707292]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepnn.back_propogation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf699a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
