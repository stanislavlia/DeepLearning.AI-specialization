{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqiHea25m7gDEGytQIzkLS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanislavlia/DeepLearning.AI-specialization/blob/main/DeepNN_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EVam5Pw8ARsz"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN():\n",
        "    def __init__(self, X, Y, dims_of_layers, activations,batch_size=None, alpha = 0.01, l2_reg = 0):\n",
        "        #dims_of_layers - list of number of units in each layer (first element - num of features in input)\n",
        "        #activations - activation function applied to each layer\n",
        "\n",
        "        #dims_of_layers[0] - n of features in input\n",
        "        #activations[0] - activation for first hidden layer\n",
        "        #we support only 3 activation funcs: linear, sigmoid, relu\n",
        "\n",
        "        #X.shape should be (n_features, m_examples)\n",
        "        #Y.shape should be (1, m_examples)\n",
        "\n",
        "        #regularization - regularization rate\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        self.m_examples = X.shape[1]\n",
        "\n",
        "        self.dims_of_layers = dims_of_layers\n",
        "        self.n_layers = len(activations)\n",
        "\n",
        "        self.activations = activations\n",
        "        self.params = dict()\n",
        "\n",
        "        #params for Adam\n",
        "        self.V = dict()\n",
        "        self.S = dict()\n",
        "\n",
        "        #so far let's treat to hyperparams for Adam Beta1 and Beta2 as constants\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.99\n",
        "        self.epsilon = 10 ** -8\n",
        "        self.t = 0 #counter for Adam\n",
        "\n",
        "        self.learning_history = []\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        if batch_size is None:\n",
        "          self.batch_size = self.m_examples\n",
        "        else:\n",
        "          self.batch_size = batch_size\n",
        "\n",
        "        #setting cache dicts for backpropogation\n",
        "\n",
        "        self.cache = dict()\n",
        "\n",
        "    def initialize_params(self):\n",
        "      #Now, we are going to use He-initialization of weights\n",
        "\n",
        "        for i in range(1, len(self.activations) + 1):\n",
        "\n",
        "            #setting parameters layer by layer\n",
        "            self.params[\"W\" + str(i)] = np.random.randn(self.dims_of_layers[i], self.dims_of_layers[i-1])\n",
        "            #multiplying by constant according to He init\n",
        "            self.params[\"W\" + str(i)] *=  np.sqrt(2 / (self.dims_of_layers[i-1]))\n",
        "\n",
        "            self.params[\"b\" + str(i)] = np.zeros((self.dims_of_layers[i], 1))\n",
        "\n",
        "            #setting params for Adam\n",
        "            self.V[\"dW\" + str(i)] = np.zeros(self.params[\"W\" + str(i)].shape)\n",
        "            self.S[\"dW\" + str(i)] = np.zeros(self.params[\"W\" + str(i)].shape)\n",
        "            self.V[\"db\" + str(i)] = np.zeros(self.params[\"b\" + str(i)].shape)\n",
        "            self.S[\"db\" + str(i)] = np.zeros(self.params[\"b\" + str(i)].shape)\n",
        "\n",
        "\n",
        "    def make_minibatches(self, batch_size):\n",
        "      #split X,y into minibatches and return list of (X_i, Y_i)\n",
        "\n",
        "      #shuffle the data\n",
        "      permuted_indices = np.random.permutation(self.X.shape[1])\n",
        "\n",
        "      # Shuffle both X and y using the same permutation for rows\n",
        "      X = self.X[:, permuted_indices]\n",
        "      Y = self.Y[:, permuted_indices]\n",
        "\n",
        "      batches = []\n",
        "      complete_batches = self.m_examples // batch_size\n",
        "\n",
        "      for k in range(0, complete_batches):\n",
        "\n",
        "        #extracting a particular slice of data\n",
        "        X_k = X[:, k * batch_size : (k + 1) * batch_size]\n",
        "        Y_k = Y[:, k * batch_size : (k + 1) * batch_size]\n",
        "\n",
        "        minibatch = (X_k, Y_k)\n",
        "        batches.append(minibatch)\n",
        "\n",
        "      #add incomplete batch in case we have remaining examples\n",
        "      if (self.m_examples % batch_size != 0):\n",
        "        X_k = X[:, complete_batches * batch_size : ]\n",
        "        Y_k = Y[:, complete_batches * batch_size : ]\n",
        "        minibatch = (X_k, Y_k)\n",
        "        batches.append(minibatch)\n",
        "\n",
        "      return (batches)\n",
        "\n",
        "\n",
        "\n",
        "    def activation(self, Z, function=\"linear\"):\n",
        "        if function == \"linear\":\n",
        "            return Z\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return Z * (Z > 0)\n",
        "\n",
        "\n",
        "    def deriv_activation(self, Z, function):\n",
        "        if function == \"linear\":\n",
        "            return 1.\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            sigm_z = self.activation(Z, \"sigmoid\")\n",
        "\n",
        "            return sigm_z *(1 - sigm_z)\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return 1. * (Z > 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propogation(self, X):\n",
        "\n",
        "        #X.shape = (n_features, m_examples)\n",
        "\n",
        "        A_prev = X\n",
        "        self.cache[\"A0\"]  = A_prev\n",
        "\n",
        "        Z_current = np.dot(self.params[\"W1\"], A_prev) + self.params[\"b1\"]\n",
        "        A_current = self.activation(Z_current, function=self.activations[0])\n",
        "\n",
        "        self.cache[\"Z1\"] = Z_current\n",
        "        self.cache[\"A1\"] = A_current\n",
        "\n",
        "        for i in range(1, len(self.activations)):\n",
        "            A_prev = A_current\n",
        "\n",
        "            #A_prev - cache[\"A\" + str(i)]\n",
        "            Z_current = np.dot(self.params[\"W\" + str(i+1)], A_prev) + self.params[\"b\" + str(i+1)]\n",
        "            A_current = self.activation(Z_current, function=self.activations[i])\n",
        "\n",
        "            #keeping values in cache for backprop\n",
        "            self.cache[\"Z\" + str(i+1)] = Z_current\n",
        "            self.cache[\"A\" + str(i+1)] = A_current\n",
        "\n",
        "\n",
        "        predictions = A_current\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def compute_reg_penalty(self):\n",
        "\n",
        "      penalty = 0\n",
        "      for l in range(1, self.n_layers + 1):\n",
        "        W_l = self.params[\"W\" + str(l)]\n",
        "        penalty += np.sum(np.square(W_l))\n",
        "\n",
        "      penalty = penalty * (self.l2_reg / (2 * self.m_examples))\n",
        "\n",
        "      return penalty\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_cost(self, predictions, cost_function): #TOFIX\n",
        "        #leave cost func as a parameter\n",
        "        #so that we can use it futher not only for classification\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #lets cut off a  tiny constant to avoid log0 problem\n",
        "            epsilon = 10 ** -15\n",
        "\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            #BinaryCrossEntropy\n",
        "\n",
        "            cost = (self.Y * np.log(predictions) +\n",
        "                    (1 - self.Y) * np.log(1 - (predictions)) ) * (-1 / self.m_examples)\n",
        "\n",
        "            cost = np.sum(cost, axis=1, keepdims=True) #sum up the columns\n",
        "\n",
        "        if cost_function == \"mse\":\n",
        "\n",
        "            cost = np.sum(((predictions - self.Y) ** 2) * (2 / self.m_examples), axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        #computing regularization penalty\n",
        "        if self.l2_reg> 0:\n",
        "\n",
        "          reg_penalty = self.compute_reg_penalty()\n",
        "          cost += reg_penalty\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def deriv_of_cost(self, Y, predictions, cost_function):\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #avoiding division by zero\n",
        "            epsilon = 10 ** -15\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            dAL = (predictions - Y) / (predictions * (1 - predictions))\n",
        "\n",
        "        if cost_function == \"mse\":\n",
        "            dAL = (predictions - Y)\n",
        "\n",
        "        return dAL\n",
        "\n",
        "\n",
        "    def back_propogation(self, Y ,predictions,  cost_func): #TOFIX\n",
        "\n",
        "        L = self.n_layers\n",
        "\n",
        "        grads_cache = dict()\n",
        "        batch_size = predictions.shape[1]\n",
        "\n",
        "        for i in range(L, 0, -1):\n",
        "            if i == L:\n",
        "                dA_i = self.deriv_of_cost(Y, predictions, cost_function=cost_func)\n",
        "            else:\n",
        "\n",
        "                dA_i = np.dot(self.params[\"W\" + str(i+1)].T, grads_cache[\"dZ\" + str(i+1)]) #ERROR MuSt be here\n",
        "                #print(\"i =\", i)\n",
        "                # print(\"W=\", self.params[\"W\" + str(i+1)] )\n",
        "                # print(\"dZ_next =\", grads_cache[\"dZ\" + str(i+1)])\n",
        "\n",
        "                # print(\"W = \", self.params[\"W\" + str(i+1)].T.shape)\n",
        "                # print(\"W shape \", )\n",
        "                # print(\"dZ shape\" + str(i),  grads_cache[\"dZ\" + str(i+1)].shape)\n",
        "                #print(\"dA\", dA_i)\n",
        "\n",
        "\n",
        "            grads_cache[\"dA\" + str(i)] = dA_i\n",
        "            activation_i = self.activations[i-1]\n",
        "\n",
        "            Z_i = self.cache[\"Z\" + str(i)]\n",
        "            A_prev = self.cache[\"A\" + str(i-1)]\n",
        "            W_i = self.params[\"W\" + str(i)]\n",
        "\n",
        "            #print(\"Activation = \", activation_i)\n",
        "            dZ_i = dA_i * self.deriv_activation(Z_i, activation_i)\n",
        "\n",
        "            #print(\"Sum of dZ_i\", np.sum(dZ_i))\n",
        "\n",
        "            #computing derivs for W, b\n",
        "\n",
        "            #L2 regularization term\n",
        "            l2_term = (self.l2_reg / batch_size) * W_i\n",
        "\n",
        "            dW_i = (1 / batch_size) * np.dot(dZ_i, A_prev.T) + l2_term\n",
        "            db_i = (1/ batch_size) * np.sum(dZ_i, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "            #storing gradients\n",
        "            grads_cache[\"dZ\" + str(i)] = dZ_i\n",
        "            grads_cache[\"dW\" + str(i)] = dW_i\n",
        "            grads_cache[\"db\" + str(i)] = db_i\n",
        "\n",
        "        return grads_cache\n",
        "\n",
        "    def update_params(self, grads, optimizer=\"gd\"): #TOFIX + opting: gd, momentum, adam\n",
        "\n",
        "        for i in range(1, self.n_layers + 1):\n",
        "\n",
        "            if optimizer == \"gd\":\n",
        "              #updating by Gradient Descent\n",
        "              self.params[\"W\" + str(i)] -=  self.alpha * grads[\"dW\" + str(i)]\n",
        "              self.params[\"b\" + str(i)] -= self.alpha * grads[\"db\" + str(i)]\n",
        "\n",
        "            #increment t-counter for Adam\n",
        "            self.t += 1\n",
        "\n",
        "            if optimizer == \"momentum\":\n",
        "              self.V[\"dW\" + str(i)] = (self.beta1 * self.V[\"dW\" + str(i)]\n",
        "                                       + (1 - self.beta1) * grads[\"dW\" + str(i)])\n",
        "\n",
        "              self.V[\"db\" + str(i)] = (self.beta1 * self.V[\"db\" + str(i)]\n",
        "                                        + (1 - self.beta1) * grads[\"db\" + str(i)])\n",
        "              #bias correction\n",
        "              self.V[\"dW\" + str(i)] /= (1 - self.beta1 ** self.t)\n",
        "              self.V[\"db\" + str(i)] /= (1 - self.beta1 ** self.t)\n",
        "              #update params by Grad Descent with momentum\n",
        "              self.params[\"W\" + str(i)] -=  self.alpha * self.V[\"dW\" + str(i)]\n",
        "              self.params[\"b\" + str(i)] -= self.alpha * self.V[\"db\" + str(i)]\n",
        "\n",
        "            if optimizer == \"adam\":\n",
        "              self.V[\"dW\" + str(i)] = (self.beta1 * self.V[\"dW\" + str(i)]\n",
        "                                       + (1 - self.beta1) * grads[\"dW\" + str(i)])\n",
        "\n",
        "              self.V[\"db\" + str(i)] = (self.beta1 * self.V[\"db\" + str(i)]\n",
        "                                        + (1 - self.beta1) * grads[\"db\" + str(i)])\n",
        "\n",
        "              self.S[\"dW\" + str(i)] = (self.beta2 * self.S[\"dW\" + str(i)]\n",
        "                                       + (1 - self.beta2) * np.square(grads[\"dW\" + str(i)]))\n",
        "              self.S[\"db\" + str(i)] = (self.beta2 * self.S[\"db\" + str(i)]\n",
        "                                       + (1 - self.beta2) * np.square(grads[\"db\" + str(i)]))\n",
        "              #bias correction\n",
        "              self.V[\"dW\" + str(i)] /= (1 - self.beta1 ** self.t)\n",
        "              self.V[\"db\" + str(i)] /= (1 - self.beta1 ** self.t)\n",
        "\n",
        "              self.S[\"dW\" + str(i)] /= (1 - self.beta2 ** self.t)\n",
        "              self.S[\"db\" + str(i)] /= (1 - self.beta2 ** self.t)\n",
        "\n",
        "              #update params\n",
        "\n",
        "              self.params[\"W\" + str(i)] -=  self.alpha * self.V[\"dW\" + str(i)] / (np.sqrt(self.S[\"dW\" + str(i)]) + self.epsilon)\n",
        "              self.params[\"b\" + str(i)] -= self.alpha * self.V[\"db\" + str(i)]  / (np.sqrt(self.S[\"db\" + str(i)]) + self.epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, epochs=100, cost_func=\"mse\", optimizer=\"gd\", to_output=15 ,batch_out=False ): #TOFIX\n",
        "\n",
        "        #fitting process\n",
        "\n",
        "        #initialize random params\n",
        "\n",
        "        history = []\n",
        "        self.initialize_params()\n",
        "\n",
        "\n",
        "        for epoch in range(0, epochs + 1):\n",
        "\n",
        "            #split into batches\n",
        "            minibatches = self.make_minibatches(self.batch_size)\n",
        "\n",
        "            to_print = bool(epoch % (epochs // to_output) == 0)\n",
        "            if to_print:\n",
        "              print(\"EPOCH# \", epoch)\n",
        "            for min_batch_num in range(len(minibatches)):\n",
        "              X_i, Y_i = minibatches[min_batch_num]\n",
        "\n",
        "              all_predictions = self.forward_propogation(self.X)\n",
        "              local_predictions = self.forward_propogation(X_i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              #computing cost function\n",
        "              cost = np.round(self.compute_cost(all_predictions, cost_func), 6)\n",
        "              history.append(cost)\n",
        "              if to_print and batch_out:\n",
        "                print(\"       batch #{}, cost = {}\".format(min_batch_num, cost))\n",
        "                print()\n",
        "\n",
        "              #computing gradients\n",
        "              grads = self.back_propogation(Y_i, local_predictions, cost_func=cost_func)\n",
        "\n",
        "              #update params using Gradient Descent\n",
        "              self.update_params(grads, optimizer)\n",
        "        self.history = history"
      ],
      "metadata": {
        "id": "p7ZiP2BVAWwA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating synthetic dataset to test implementation**"
      ],
      "metadata": {
        "id": "_9ZqQGqvGyrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "scrolled": true,
        "id": "ab8bcb8b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Trying on a data\n",
        "\n",
        "np.random.seed(2004)\n",
        "\n",
        "# Generate data for not admitted students\n",
        "mean_not_admitted = [70, 75]\n",
        "cov_not_admitted = [[100, 50], [50, 100]]\n",
        "not_admitted = np.random.multivariate_normal(mean_not_admitted, cov_not_admitted, size=200)\n",
        "\n",
        "# Generate data for admitted students\n",
        "mean_admitted = [90, 90]\n",
        "cov_admitted = [[100, 50], [50, 100]]\n",
        "admitted = np.random.multivariate_normal(mean_admitted, cov_admitted, size=200)\n",
        "\n",
        "# Combine the data\n",
        "X = np.vstack((not_admitted, admitted)) / 100\n",
        "y = np.hstack((np.zeros(200), np.ones(200)))\n",
        "\n",
        "\n",
        "\n",
        "# Plot the generated data\n",
        "\n",
        "# plt.figure(figsize=(11, 8))\n",
        "# plt.scatter(not_admitted[:, 0], not_admitted[:, 1], label=\"Not Admitted\")\n",
        "# plt.scatter(admitted[:, 0], admitted[:, 1], label=\"Admitted\")\n",
        "# plt.xlabel(\"Test Score\")\n",
        "# plt.ylabel(\"Grade\")\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "#transform data for our model\n",
        "X = X.T\n",
        "y = y.reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [X.shape[0], 5, 1]\n",
        "activations = [\"relu\", \"sigmoid\"]\n",
        "\n",
        "\n",
        "deepnn = DeepNN(X, y, dims, activations, batch_size=100, alpha=0.01)\n",
        "deepnn.fit(epochs=10,to_output=5, optimizer=\"momentum\",batch_out=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxeruitdG7A0",
        "outputId": "45efe4e5-4944-43b2-e521-33588beff8ad"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH#  0\n",
            "       batch #0, cost = [[0.501166]]\n",
            "\n",
            "       batch #1, cost = [[0.501162]]\n",
            "\n",
            "       batch #2, cost = [[0.501144]]\n",
            "\n",
            "       batch #3, cost = [[0.501101]]\n",
            "\n",
            "EPOCH#  2\n",
            "       batch #0, cost = [[0.500412]]\n",
            "\n",
            "       batch #1, cost = [[0.500206]]\n",
            "\n",
            "       batch #2, cost = [[0.499992]]\n",
            "\n",
            "       batch #3, cost = [[0.499778]]\n",
            "\n",
            "EPOCH#  4\n",
            "       batch #0, cost = [[0.498829]]\n",
            "\n",
            "       batch #1, cost = [[0.498671]]\n",
            "\n",
            "       batch #2, cost = [[0.498527]]\n",
            "\n",
            "       batch #3, cost = [[0.498394]]\n",
            "\n",
            "EPOCH#  6\n",
            "       batch #0, cost = [[0.497883]]\n",
            "\n",
            "       batch #1, cost = [[0.497805]]\n",
            "\n",
            "       batch #2, cost = [[0.497734]]\n",
            "\n",
            "       batch #3, cost = [[0.497671]]\n",
            "\n",
            "EPOCH#  8\n",
            "       batch #0, cost = [[0.497424]]\n",
            "\n",
            "       batch #1, cost = [[0.497386]]\n",
            "\n",
            "       batch #2, cost = [[0.49735]]\n",
            "\n",
            "       batch #3, cost = [[0.497318]]\n",
            "\n",
            "EPOCH#  10\n",
            "       batch #0, cost = [[0.497192]]\n",
            "\n",
            "       batch #1, cost = [[0.497171]]\n",
            "\n",
            "       batch #2, cost = [[0.497152]]\n",
            "\n",
            "       batch #3, cost = [[0.497134]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "train_preds = np.round(deepnn.forward_propogation(X))"
      ],
      "metadata": {
        "id": "Rt4WhRrNHAr8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(train_preds.T, y.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8yFdDOrIGu2",
        "outputId": "7121afa4-1dbc-47ce-8976-f402fc38e25d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.86"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-o6MKK2BUo5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}