{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stanislavlia/DeepLearning.AI-specialization/blob/main/DeepNN2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EVam5Pw8ARsz"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p7ZiP2BVAWwA"
   },
   "outputs": [],
   "source": [
    "class DeepNN():\n",
    "    def __init__(self, X, Y, dims_of_layers, activations, alpha = 0.01, l2_reg = 0):\n",
    "        #dims_of_layers - list of number of units in each layer (first element - num of features in input)\n",
    "        #activations - activation function applied to each layer\n",
    "\n",
    "        #dims_of_layers[0] - n of features in input\n",
    "        #activations[0] - activation for first hidden layer\n",
    "        #we support only 3 activation funcs: linear, sigmoid, relu\n",
    "\n",
    "        #X.shape should be (n_features, m_examples)\n",
    "        #Y.shape should be (1, m_examples)\n",
    "\n",
    "        #regularization - regularization rate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.m_examples = X.shape[1]\n",
    "\n",
    "        self.dims_of_layers = dims_of_layers\n",
    "        self.n_layers = len(activations)\n",
    "\n",
    "        self.activations = activations\n",
    "        self.params = dict()\n",
    "\n",
    "        self.learning_history = []\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        #setting cache dicts for backpropogation\n",
    "\n",
    "        self.cache = dict()\n",
    "\n",
    "    def initialize_params(self):\n",
    "      #Now, we are going to use He-initialization of weights\n",
    "\n",
    "        for i in range(1, len(self.activations) + 1):\n",
    "\n",
    "            #setting parameters layer by layer\n",
    "            self.params[\"W\" + str(i)] = np.random.randn(self.dims_of_layers[i], self.dims_of_layers[i-1])\n",
    "            #multiplying by constant according to He init\n",
    "            self.params[\"W\" + str(i)] *=  np.sqrt(2 / (self.dims_of_layers[i-1]))\n",
    "\n",
    "\n",
    "            self.params[\"b\" + str(i)] = np.zeros((self.dims_of_layers[i], 1))\n",
    "\n",
    "\n",
    "    def make_minibatches(self, batch_size):\n",
    "      #split X,y into minibatches and return list of (X_i, Y_i)\n",
    "\n",
    "      batches = []\n",
    "      complete_batches = self.m_examples // batch_size\n",
    "\n",
    "      for k in range(0, complete_batches):\n",
    "\n",
    "        #extracting a particular slice of data\n",
    "        X_k = self.X[:, k * batch_size : (k + 1) * batch_size]\n",
    "        Y_k = self.Y[:, k * batch_size : (k + 1) * batch_size]\n",
    "\n",
    "        minibatch = (X_k, Y_k)\n",
    "        batches.append(minibatch)\n",
    "\n",
    "      #add incomplete batch in case we have remaining examples\n",
    "      if (self.m_examples % batch_size != 0):\n",
    "        X_k = self.X[:, complete_batches * batch_size : ]\n",
    "        Y_k = self.Y[:, complete_batches * batch_size : ]\n",
    "        minibatch = (X_k, Y_k)\n",
    "        batches.append(minibatch)\n",
    "\n",
    "      return (batches)\n",
    "\n",
    "\n",
    "\n",
    "    def activation(self, Z, function=\"linear\"):\n",
    "        if function == \"linear\":\n",
    "            return Z\n",
    "\n",
    "        if function == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "        if function == \"relu\":\n",
    "            return Z * (Z > 0)\n",
    "\n",
    "\n",
    "    def deriv_activation(self, Z, function):\n",
    "        if function == \"linear\":\n",
    "            return 1.\n",
    "\n",
    "        if function == \"sigmoid\":\n",
    "            sigm_z = self.activation(Z, \"sigmoid\")\n",
    "\n",
    "            return sigm_z *(1 - sigm_z)\n",
    "\n",
    "        if function == \"relu\":\n",
    "            return 1. * (Z > 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward_propogation(self, X): #TOFIX\n",
    "\n",
    "        #X.shape = (n_features, m_examples)\n",
    "\n",
    "        A_prev = X\n",
    "        self.cache[\"A0\"]  = A_prev\n",
    "\n",
    "        Z_current = np.dot(self.params[\"W1\"], A_prev) + self.params[\"b1\"]\n",
    "        A_current = self.activation(Z_current, function=self.activations[0])\n",
    "\n",
    "        self.cache[\"Z1\"] = Z_current\n",
    "        self.cache[\"A1\"] = A_current\n",
    "\n",
    "        for i in range(1, len(self.activations)):\n",
    "            A_prev = A_current\n",
    "\n",
    "            #A_prev - cache[\"A\" + str(i)]\n",
    "            Z_current = np.dot(self.params[\"W\" + str(i+1)], A_prev) + self.params[\"b\" + str(i+1)]\n",
    "            A_current = self.activation(Z_current, function=self.activations[i])\n",
    "\n",
    "            #keeping values in cache for backprop\n",
    "            self.cache[\"Z\" + str(i+1)] = Z_current\n",
    "            self.cache[\"A\" + str(i+1)] = A_current\n",
    "\n",
    "\n",
    "        predictions = A_current\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def compute_reg_penalty(self): #TOFIX\n",
    "\n",
    "      penalty = 0\n",
    "      for l in range(1, self.n_layers + 1):\n",
    "        W_l = self.params[\"W\" + str(l)]\n",
    "        penalty += np.sum(np.square(W_l))\n",
    "\n",
    "      penalty = penalty * (self.l2_reg / (2 * self.m_examples))\n",
    "\n",
    "      return penalty\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_cost(self, predictions, cost_function): #TOFIX\n",
    "        #leave cost func as a parameter\n",
    "        #so that we can use it futher not only for classification\n",
    "\n",
    "        if cost_function == \"cross_entropy\":\n",
    "\n",
    "            #lets cut off a  tiny constant to avoid log0 problem\n",
    "            epsilon = 10 ** -15\n",
    "\n",
    "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
    "\n",
    "            #BinaryCrossEntropy\n",
    "\n",
    "            cost = (self.Y * np.log(predictions) +\n",
    "                    (1 - self.Y) * np.log(1 - (predictions)) ) * (-1 / self.m_examples)\n",
    "\n",
    "            cost = np.sum(cost, axis=1, keepdims=True) #sum up the columns\n",
    "\n",
    "        if cost_function == \"mse\":\n",
    "\n",
    "            cost = np.sum(((predictions - self.Y) ** 2) * (2 / self.m_examples), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        #computing regularization penalty\n",
    "        if self.l2_reg> 0:\n",
    "\n",
    "          reg_penalty = self.compute_reg_penalty()\n",
    "          cost += reg_penalty\n",
    "\n",
    "\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def deriv_of_cost(self, predictions, cost_function):\n",
    "\n",
    "        if cost_function == \"cross_entropy\":\n",
    "\n",
    "            #avoiding division by zero\n",
    "            epsilon = 10 ** -15\n",
    "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
    "\n",
    "            dAL = (predictions - self.Y) / (predictions * (1 - predictions))\n",
    "\n",
    "        if cost_function == \"mse\":\n",
    "            dAL = (predictions - self.Y)\n",
    "\n",
    "        return dAL\n",
    "\n",
    "\n",
    "    def back_propogation(self, predictions, cost_func): #TOFIX\n",
    "\n",
    "        L = self.n_layers\n",
    "\n",
    "        grads_cache = dict()\n",
    "\n",
    "        for i in range(L, 0, -1):\n",
    "            if i == L:\n",
    "                dA_i = self.deriv_of_cost(predictions, cost_function=cost_func)\n",
    "            else:\n",
    "\n",
    "                dA_i = np.dot(self.params[\"W\" + str(i+1)].T, grads_cache[\"dZ\" + str(i+1)]) #ERROR MuSt be here\n",
    "                #print(\"i =\", i)\n",
    "                # print(\"W=\", self.params[\"W\" + str(i+1)] )\n",
    "                # print(\"dZ_next =\", grads_cache[\"dZ\" + str(i+1)])\n",
    "\n",
    "                # print(\"W = \", self.params[\"W\" + str(i+1)].T.shape)\n",
    "                # print(\"W shape \", )\n",
    "                # print(\"dZ shape\" + str(i),  grads_cache[\"dZ\" + str(i+1)].shape)\n",
    "                #print(\"dA\", dA_i)\n",
    "\n",
    "\n",
    "            grads_cache[\"dA\" + str(i)] = dA_i\n",
    "            activation_i = self.activations[i-1]\n",
    "\n",
    "            Z_i = self.cache[\"Z\" + str(i)]\n",
    "            A_prev = self.cache[\"A\" + str(i-1)]\n",
    "            W_i = self.params[\"W\" + str(i)]\n",
    "\n",
    "            #print(\"Activation = \", activation_i)\n",
    "            dZ_i = dA_i * self.deriv_activation(Z_i, activation_i)\n",
    "\n",
    "            #print(\"Sum of dZ_i\", np.sum(dZ_i))\n",
    "\n",
    "            #computing derivs for W, b\n",
    "\n",
    "            #L2 regularization term\n",
    "            l2_term = (self.l2_reg / self.m_examples) * W_i\n",
    "\n",
    "            dW_i = (1 / self.m_examples) * np.dot(dZ_i, A_prev.T) + l2_term\n",
    "            db_i = (1/ self.m_examples) * np.sum(dZ_i, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "            #storing gradients\n",
    "            grads_cache[\"dZ\" + str(i)] = dZ_i\n",
    "            grads_cache[\"dW\" + str(i)] = dW_i\n",
    "            grads_cache[\"db\" + str(i)] = db_i\n",
    "\n",
    "        return grads_cache\n",
    "\n",
    "    def update_params(self, grads): #TOFIX + opting: gd, momentum, adam\n",
    "\n",
    "        for i in range(1, self.n_layers + 1):\n",
    "\n",
    "            #updating by Gradient Descent\n",
    "            self.params[\"W\" + str(i)] -=  self.alpha * grads[\"dW\" + str(i)]\n",
    "            self.params[\"b\" + str(i)] -= self.alpha * grads[\"db\" + str(i)]\n",
    "\n",
    "\n",
    "    def fit(self, epochs=100, cost_func=\"mse\", debug=False): #TOFIX\n",
    "\n",
    "        #fitting process\n",
    "\n",
    "        #initialize random params\n",
    "\n",
    "        history = []\n",
    "        self.initialize_params()\n",
    "\n",
    "\n",
    "        for epoch in range(0, epochs + 1):\n",
    "\n",
    "            predictions = self.forward_propogation(self.X)\n",
    "\n",
    "            #computing cost function\n",
    "            cost = np.round(self.compute_cost(predictions, cost_func), 6)\n",
    "            history.append(cost)\n",
    "\n",
    "\n",
    "            if (epoch % max(1 , (epochs // 20)) == 0):\n",
    "              print(\"Epoch #{},  {} == {}\".format(epoch, cost_func, cost))\n",
    "\n",
    "            #computing gradients\n",
    "            grads = self.back_propogation(predictions, cost_func=cost_func)\n",
    "            if debug == True:\n",
    "              print(grads)\n",
    "\n",
    "            #update params using Gradient Descent\n",
    "            self.update_params(grads)\n",
    "\n",
    "        self.history = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhpZr-ygBn7K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP1wbT9c5CfF4/iSK9WjBb4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
