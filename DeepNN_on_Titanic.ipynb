{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLNOr9n5o3CeyBgtd70muZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanislavlia/DeepLearning.AI-specialization/blob/main/DeepNN_on_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "H2P-2cFOEV4m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing of Titanic Dataset**"
      ],
      "metadata": {
        "id": "qSPRtgQWI5UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"sample_data/train.csv\")\n",
        "#some data definetly useless for predicion such as ticket number, name\n",
        "df = df.drop([\"Ticket\", \"PassengerId\", \"Name\", \"Cabin\"], axis=1)\n",
        "\n",
        "df[\"Sex\"] = df[\"Sex\"].replace({\"male\" : 0, \"female\" : 1})\n",
        "\n",
        "df[\"Embarked\"] = df[\"Embarked\"].replace({\"S\" : 0, \"C\" : 1, \"Q\": 2})"
      ],
      "metadata": {
        "id": "Dn5rEblnE8Xl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "X = df.drop(\"Survived\", axis=1).values\n",
        "y = df[\"Survived\"].values"
      ],
      "metadata": {
        "id": "jn4DfD6nFJ6v"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling Features\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "X_scaled = std_scaler.fit_transform(X)\n",
        "\n",
        "#Splitting into train/test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.75, random_state=1)"
      ],
      "metadata": {
        "id": "JnNQBZ0gFSrk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforming for shape of our implementation\n",
        "\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "\n",
        "y_train = y_train.reshape(-1, 1).T\n",
        "y_test = y_test.reshape(-1, 1).T"
      ],
      "metadata": {
        "id": "LM4ePNmGFXJB"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#My implementation\n",
        "\n",
        "class DeepNN():\n",
        "    def __init__(self, X, Y, dims_of_layers, activations, alpha = 0.01):\n",
        "        #dims_of_layers - list of number of units in each layer (first element - num of features in input)\n",
        "        #activations - activation function applied to each layer\n",
        "\n",
        "        #dims_of_layers[0] - n of features in input\n",
        "        #activations[0] - activation for first hidden layer\n",
        "        #we support only 3 activation funcs: linear, sigmoid, relu\n",
        "\n",
        "        #X.shape should be (n_features, m_examples)\n",
        "        #Y.shape should be (1, m_examples)\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        self.m_examples = X.shape[1]\n",
        "\n",
        "        self.dims_of_layers = dims_of_layers\n",
        "        self.n_layers = len(activations)\n",
        "\n",
        "        self.activations = activations\n",
        "        self.params = dict()\n",
        "\n",
        "        self.learning_history = []\n",
        "        self.alpha = alpha\n",
        "\n",
        "        #setting cache dicts for backpropogation\n",
        "\n",
        "        self.cache = dict()\n",
        "\n",
        "    def initialize_params(self):\n",
        "\n",
        "        for i in range(1, len(self.activations) + 1):\n",
        "\n",
        "            #setting parameters layer by layer\n",
        "            self.params[\"W\" + str(i)] = np.random.randn(self.dims_of_layers[i], self.dims_of_layers[i-1])\n",
        "            self.params[\"b\" + str(i)] = np.zeros((self.dims_of_layers[i], 1))\n",
        "\n",
        "\n",
        "\n",
        "    def activation(self, Z, function=\"linear\"):\n",
        "        if function == \"linear\":\n",
        "            return Z\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return Z * (Z > 0)\n",
        "\n",
        "\n",
        "    def deriv_activation(self, Z, function):\n",
        "        if function == \"linear\":\n",
        "            return 1.\n",
        "\n",
        "        if function == \"sigmoid\":\n",
        "            sigm_z = self.activation(Z, \"sigmoid\")\n",
        "\n",
        "            return sigm_z *(1 - sigm_z)\n",
        "\n",
        "        if function == \"relu\":\n",
        "            return 1. * (Z > 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward_propogation(self, X):\n",
        "\n",
        "        #X.shape = (n_features, m_examples)\n",
        "\n",
        "        A_prev = X\n",
        "        self.cache[\"A0\"]  = A_prev\n",
        "\n",
        "        Z_current = np.dot(self.params[\"W1\"], A_prev) + self.params[\"b1\"]\n",
        "        A_current = self.activation(Z_current, function=self.activations[0])\n",
        "\n",
        "        self.cache[\"Z1\"] = Z_current\n",
        "        self.cache[\"A1\"] = A_current\n",
        "\n",
        "        for i in range(1, len(self.activations)):\n",
        "            A_prev = A_current\n",
        "\n",
        "            #A_prev - cache[\"A\" + str(i)]\n",
        "            Z_current = np.dot(self.params[\"W\" + str(i+1)], A_prev) + self.params[\"b\" + str(i+1)]\n",
        "            A_current = self.activation(Z_current, function=self.activations[i])\n",
        "\n",
        "            #keeping values in cache for backprop\n",
        "            self.cache[\"Z\" + str(i+1)] = Z_current\n",
        "            self.cache[\"A\" + str(i+1)] = A_current\n",
        "\n",
        "\n",
        "        predictions = A_current\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def compute_cost(self, predictions, cost_function=\"cross_entropy\"):\n",
        "        #leave cost func as a parameter\n",
        "        #so that we can use it futher not only for classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #lets cut off a  tiny constant to avoid log0 problem\n",
        "            epsilon = 10 ** -15\n",
        "\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            #BinaryCrossEntropy\n",
        "\n",
        "            cost = (self.Y * np.log(predictions) +\n",
        "                    (1 - self.Y) * np.log(1 - (predictions)) ) * (-1 / self.m_examples)\n",
        "\n",
        "            cost = np.sum(cost, axis=1, keepdims=True) #sum up the columns\n",
        "\n",
        "        #we will have more cost functions here later...\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def deriv_of_cost(self, predictions, cost_function=\"cross_entropy\"):\n",
        "\n",
        "        if cost_function == \"cross_entropy\":\n",
        "\n",
        "            #avoiding division by zero\n",
        "            epsilon = 10 ** -15\n",
        "            predictions = np.clip(predictions, epsilon, 1-epsilon)\n",
        "\n",
        "            dAL = (predictions - self.Y) / (predictions * (1 - predictions))\n",
        "\n",
        "        #we will have more cost functions here later...\n",
        "\n",
        "        return dAL\n",
        "\n",
        "\n",
        "    def back_propogation(self, predictions, cost_func=\"cross_entropy\"):\n",
        "\n",
        "        L = self.n_layers\n",
        "\n",
        "        grads_cache = dict()\n",
        "\n",
        "        for i in range(L, 0, -1):\n",
        "            if i == L:\n",
        "                dA_i = self.deriv_of_cost(predictions, cost_function=cost_func)\n",
        "            else:\n",
        "\n",
        "                dA_i = np.dot(self.params[\"W\" + str(i+1)].T, grads_cache[\"dZ\" + str(i+1)]) #ERROR MuSt be here\n",
        "                #print(\"i =\", i)\n",
        "                # print(\"W=\", self.params[\"W\" + str(i+1)] )\n",
        "                # print(\"dZ_next =\", grads_cache[\"dZ\" + str(i+1)])\n",
        "\n",
        "                # print(\"W = \", self.params[\"W\" + str(i+1)].T.shape)\n",
        "                # print(\"W shape \", )\n",
        "                # print(\"dZ shape\" + str(i),  grads_cache[\"dZ\" + str(i+1)].shape)\n",
        "                #print(\"dA\", dA_i)\n",
        "\n",
        "\n",
        "            grads_cache[\"dA\" + str(i)] = dA_i\n",
        "            activation_i = self.activations[i-1]\n",
        "\n",
        "            Z_i = self.cache[\"Z\" + str(i)]\n",
        "            A_prev = self.cache[\"A\" + str(i-1)]\n",
        "            W_i = self.params[\"W\" + str(i)]\n",
        "\n",
        "            #print(\"Activation = \", activation_i)\n",
        "            dZ_i = dA_i * self.deriv_activation(Z_i, activation_i)\n",
        "\n",
        "            #print(\"Sum of dZ_i\", np.sum(dZ_i))\n",
        "\n",
        "            #computing derivs for W, b\n",
        "            dW_i = (1 / self.m_examples) * np.dot(dZ_i, A_prev.T)\n",
        "            db_i = (1/ self.m_examples) * np.sum(dZ_i, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "            #storing gradients\n",
        "            grads_cache[\"dZ\" + str(i)] = dZ_i\n",
        "            grads_cache[\"dW\" + str(i)] = dW_i\n",
        "            grads_cache[\"db\" + str(i)] = db_i\n",
        "\n",
        "        return grads_cache\n",
        "\n",
        "    def update_params(self, grads):\n",
        "\n",
        "        for i in range(1, self.n_layers + 1):\n",
        "\n",
        "            #updating by Gradient Descent\n",
        "            self.params[\"W\" + str(i)] -=  self.alpha * grads[\"dW\" + str(i)]\n",
        "            self.params[\"b\" + str(i)] -= self.alpha * grads[\"db\" + str(i)]\n",
        "\n",
        "\n",
        "    def fit(self, epochs=100, cost_func=\"cross_entropy\", debug=False):\n",
        "\n",
        "        #fitting process\n",
        "\n",
        "        #initialize random params\n",
        "\n",
        "        history = []\n",
        "        self.initialize_params()\n",
        "\n",
        "\n",
        "        for epoch in range(0, epochs + 1):\n",
        "\n",
        "            predictions = self.forward_propogation(self.X)\n",
        "\n",
        "            #computing cost function\n",
        "            cost = np.round(self.compute_cost(predictions, cost_func), 6)\n",
        "            history.append(cost)\n",
        "\n",
        "\n",
        "            if (epoch % max(1 , (epochs // 20)) == 0):\n",
        "              print(\"Epoch #{},  {} == {}\".format(epoch, cost_func, cost))\n",
        "\n",
        "            #computing gradients\n",
        "            grads = self.back_propogation(predictions, cost_func=\"cross_entropy\")\n",
        "            if debug == True:\n",
        "              print(grads)\n",
        "\n",
        "            #update params using Gradient Descent\n",
        "            self.update_params(grads)\n",
        "\n",
        "        self.history = history"
      ],
      "metadata": {
        "id": "UrO94KRZFYsK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building structure for Deep Neural Network**"
      ],
      "metadata": {
        "id": "FXjKeFXUF4KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_units = [X_train.shape[0], 10, 5, 1]\n",
        "activations = [\"relu\", \"relu\", \"sigmoid\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "3eB7F0XjF0cJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepnn = DeepNN(X_train, y_train, n_units, activations, alpha = 0.01)\n",
        "deepnn.fit(15000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSMkz0PFGVEV",
        "outputId": "55ece847-d556-414c-cc8a-989bc5351815"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch #0,  cross_entropy == [[9.272832]]\n",
            "Epoch #750,  cross_entropy == [[0.551106]]\n",
            "Epoch #1500,  cross_entropy == [[0.51706]]\n",
            "Epoch #2250,  cross_entropy == [[0.501535]]\n",
            "Epoch #3000,  cross_entropy == [[0.460147]]\n",
            "Epoch #3750,  cross_entropy == [[0.438692]]\n",
            "Epoch #4500,  cross_entropy == [[0.428698]]\n",
            "Epoch #5250,  cross_entropy == [[0.421387]]\n",
            "Epoch #6000,  cross_entropy == [[0.41542]]\n",
            "Epoch #6750,  cross_entropy == [[0.410508]]\n",
            "Epoch #7500,  cross_entropy == [[0.406717]]\n",
            "Epoch #8250,  cross_entropy == [[0.403265]]\n",
            "Epoch #9000,  cross_entropy == [[0.400219]]\n",
            "Epoch #9750,  cross_entropy == [[0.397599]]\n",
            "Epoch #10500,  cross_entropy == [[0.395319]]\n",
            "Epoch #11250,  cross_entropy == [[0.39326]]\n",
            "Epoch #12000,  cross_entropy == [[0.391495]]\n",
            "Epoch #12750,  cross_entropy == [[0.389836]]\n",
            "Epoch #13500,  cross_entropy == [[0.388119]]\n",
            "Epoch #14250,  cross_entropy == [[0.386261]]\n",
            "Epoch #15000,  cross_entropy == [[0.384499]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.round(deepnn.forward_propogation(X_test)).T\n",
        "\n",
        "test_accuracy = accuracy_score(y_test.T, predictions)\n",
        "test_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSS9hpSlGm2K",
        "outputId": "38523519-5f85-48c7-b085-42749dc3e6d7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8071748878923767"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eg-aJO9LIa91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}